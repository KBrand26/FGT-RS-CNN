{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.7.0 and strictly below 2.10.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'train_merged' from 'train_models' (/tf/notebooks/src/Galaxy_Networks/../train_models.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      8\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_merged\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_prep\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m norm_image\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluate_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_cm, process_labels, calc_precision_recall, calc_f1\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'train_merged' from 'train_models' (/tf/notebooks/src/Galaxy_Networks/../train_models.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from functools import partial\n",
    "from tensorflow import keras\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from train_models import train_merged, probe_dir\n",
    "from data_prep import norm_image\n",
    "from evaluate_models import plot_cm, process_labels, calc_precision_recall, calc_f1\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('../../data/galaxy_X.npy')\n",
    "y = np.load('../../data/galaxy_y.npy')\n",
    "y_aux = np.load('../../data/galaxy_y_aux.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set seeds to be able to reproduce network\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_tmp_train, X_test1, y_aux_tmp_train, y_aux_test1, y_tmp_train, y_test = train_test_split(X, y_aux, y, test_size=0.1, train_size=0.9, random_state=42, shuffle=True, stratify=y)\n",
    "X_train1, X_val1, y_aux_train1, y_aux_val1, y_train, y_val = train_test_split(X_tmp_train, y_aux_tmp_train, y_tmp_train, test_size=0.11, train_size=0.89, random_state=42, shuffle=True, stratify=y_tmp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply normalization to images\n",
    "def normalize_images(X):\n",
    "    return np.array(list(map(normalize_image, X)))\n",
    "    \n",
    "def normalize_image(img):\n",
    "    bot = np.min(img)\n",
    "    top = np.max(img)\n",
    "    norm = (img - bot)/(top - bot)\n",
    "    return norm\n",
    "\n",
    "X_train = normalize_images(X_train1)\n",
    "X_val = normalize_images(X_val1)\n",
    "X_test = normalize_images(X_test1)\n",
    "\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_val = X_val[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features to allow for sigmoid activation\n",
    "def normalize_targets(y):\n",
    "    bot = np.min(y, axis=0)\n",
    "    top = np.max(y, axis=0)\n",
    "    norm = (y - bot)/(top - bot)\n",
    "    return norm\n",
    "\n",
    "y_aux_train = normalize_targets(y_aux_train1)\n",
    "y_aux_val = normalize_targets(y_aux_val1)\n",
    "y_aux_test = normalize_targets(y_aux_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Even weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evenly weighted heads\n",
    "runs = 20\n",
    "total_loss = 0\n",
    "total_lbl_entropy = 0\n",
    "total_bent_entropy = 0\n",
    "total_fr_mse = 0\n",
    "total_cores_mse = 0\n",
    "total_size_mse = 0\n",
    "total_lbl_acc = 0\n",
    "total_bent_acc = 0\n",
    "total_fr_mae = 0\n",
    "total_cores_mae = 0\n",
    "total_size_mae = 0\n",
    "elapsed = 0\n",
    "\n",
    "# Execute a few runs of training the network\n",
    "for run in range(1,runs+1):\n",
    "    start = time.time()\n",
    "    tmp_loss, tmp_lbl_entropy, tmp_bent_entropy, tmp_fr_mse, tmp_cores_mse, tmp_size_mse, tmp_lbl_acc, tmp_bent_acc, tmp_fr_mae, tmp_cores_mae, tmp_size_mae = train_merged(X_train, y_train, y_aux_train,\n",
    "                                          X_val, y_val, y_aux_val, X_test, y_test, y_aux_test, 0.5, 0.5, run)\n",
    "    keras.backend.clear_session()\n",
    "    end = time.time()\n",
    "    elapsed += (end - start)\n",
    "    total_loss += tmp_loss\n",
    "    total_lbl_entropy += tmp_lbl_entropy\n",
    "    total_bent_entropy += tmp_bent_entropy\n",
    "    total_fr_mse += tmp_fr_mse\n",
    "    total_cores_mse += tmp_cores_mse\n",
    "    total_size_mse += tmp_size_mse\n",
    "    total_lbl_acc += tmp_lbl_acc\n",
    "    total_bent_acc += tmp_bent_acc\n",
    "    total_fr_mae += tmp_fr_mae\n",
    "    total_cores_mae += tmp_cores_mae\n",
    "    total_size_mae += tmp_size_mae\n",
    "avg_loss = total_loss/runs\n",
    "avg_lbl_entropy = total_lbl_entropy/runs\n",
    "avg_bent_entropy = total_bent_entropy/runs\n",
    "avg_fr_mse = total_fr_mse/runs\n",
    "avg_cores_mse = total_cores_mse/runs\n",
    "avg_size_mse = total_size_mse/runs\n",
    "avg_lbl_acc = total_lbl_acc/runs\n",
    "avg_bent_acc = total_bent_acc/runs\n",
    "avg_fr_mae = total_fr_mae/runs\n",
    "avg_cores_mae = total_cores_mae/runs\n",
    "avg_size_mae = total_size_mae/runs\n",
    "avg_elapsed = elapsed/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 20\n",
    "epochs_log = []\n",
    "for run in range(1,runs+1):\n",
    "    ea = EventAccumulator(f'../../lr_logs/merged_0.5_0.5_run{run}/train')\n",
    "    ea.Reload()\n",
    "    ct_loss, epochs_loss, loss = zip(*ea.Tensors('epoch_loss'))\n",
    "    epochs_loss = np.asarray(epochs_loss)\n",
    "    epochs_log.append(epochs_loss[-1] + 1)\n",
    "\n",
    "epochs_log = np.array(epochs_log)\n",
    "avg_epochs = np.mean(epochs_log)\n",
    "min_idx = np.argmin(epochs_log) + 1\n",
    "min_epochs = epochs_log[min_idx - 1]\n",
    "max_idx = np.argmax(epochs_log) + 1\n",
    "max_epochs = epochs_log[max_idx - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = (avg_fr_mse + avg_cores_mse + avg_size_mse)/3\n",
    "avg_mae = (avg_fr_mae + avg_cores_mae + avg_size_mae)/3\n",
    "tmp = np.array([avg_lbl_entropy, avg_bent_entropy, avg_fr_mse, avg_cores_mse, avg_size_mse, avg_lbl_acc, avg_bent_acc, avg_fr_mae, avg_cores_mae, avg_size_mae, avg_mse, avg_mae, avg_loss, avg_elapsed, avg_epochs, max_epochs, max_idx, min_epochs, min_idx])\n",
    "\n",
    "probe_dir('../../results/')\n",
    "np.save('../../results/merged_even_results.npy', tmp)\n",
    "tmp = np.load('../../results/merged_even_results.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average classification entropy: {tmp[0]}\")\n",
    "print(f\"Average bent entropy: {tmp[1]}\")\n",
    "print(f\"Average FR ratio MSE: {tmp[2]}\")\n",
    "print(f\"Average core count MSE: {tmp[3]}\")\n",
    "print(f\"Average core ratio MSE: {tmp[4]}\")\n",
    "print(f\"Average classification accuracy: {tmp[5]}\")\n",
    "print(f\"Average bent accuracy: {tmp[6]}\")\n",
    "print(f\"Average FR ratio MAE: {tmp[7]}\")\n",
    "print(f\"Average core count MAE: {tmp[8]}\")\n",
    "print(f\"Average core ratio MAE: {tmp[9]}\")\n",
    "print(f\"Average overall MSE: {tmp[10]}\")\n",
    "print(f\"Average overall MAE: {tmp[11]}\")\n",
    "print(f\"Average loss: {tmp[12]}\")\n",
    "print(f\"Average time taken: {tmp[13]}\")\n",
    "print(f\"Average epochs taken: {tmp[14]}\")\n",
    "print(f\"Maximum number of epochs taken was {tmp[15]} at run {tmp[16]}\")\n",
    "print(f\"Minimum number of epochs taken was {tmp[17]} at run {tmp[18]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_class_pred(preds):\n",
    "    return preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bent_precs, bent_recalls, bent_f1s = [], [], []\n",
    "comp_precs, comp_recalls, comp_f1s = [], [], []\n",
    "fri_precs, fri_recalls, fri_f1s = [], [], []\n",
    "frii_precs, frii_recalls, frii_f1s = [], [], []\n",
    "macro_f1s = []\n",
    "average_cm = np.zeros((4, 4))\n",
    "runs = 20\n",
    "for run in range(1,runs+1):\n",
    "    best_model = keras.models.load_model(f\"../../models/merged_0.5_0.5_model{run}.h5\")\n",
    "    test_pred = extract_class_pred(best_model.predict(X_test))\n",
    "    new_test_true, new_test_pred = process_labels(y_test, test_pred)\n",
    "    cm = confusion_matrix(new_test_true, new_test_pred)\n",
    "    average_cm += cm\n",
    "    bent_prec, bent_recall = calc_precision_recall(cm, 0)\n",
    "    bent_precs.append(bent_prec)\n",
    "    bent_recalls.append(bent_recall)\n",
    "    bent_f1s.append(calc_f1(bent_prec, bent_recall))\n",
    "    comp_prec, comp_recall = calc_precision_recall(cm, 1)\n",
    "    comp_precs.append(comp_prec)\n",
    "    comp_recalls.append(comp_recall)\n",
    "    comp_f1s.append(calc_f1(comp_prec, comp_recall))\n",
    "    fri_prec, fri_recall = calc_precision_recall(cm, 2)\n",
    "    fri_precs.append(fri_prec)\n",
    "    fri_recalls.append(fri_recall)\n",
    "    fri_f1s.append(calc_f1(fri_prec, fri_recall))\n",
    "    frii_prec, frii_recall = calc_precision_recall(cm, 3)\n",
    "    frii_precs.append(frii_prec)\n",
    "    frii_recalls.append(frii_recall)\n",
    "    frii_f1s.append(calc_f1(frii_prec, frii_recall))\n",
    "    macro_f1s.append((bent_f1s[-1] + comp_f1s[-1] + fri_f1s[-1] + frii_f1s[-1])/4)\n",
    "\n",
    "average_cm = average_cm/runs\n",
    "print(f'Average bent precision: {np.mean(bent_precs)}')\n",
    "print(f'Average bent recall: {np.mean(bent_recalls)}')\n",
    "print(f'Average bent F1: {np.mean(bent_f1s)}')\n",
    "print(f'Average comp precision: {np.mean(comp_precs)}')\n",
    "print(f'Average comp recall: {np.mean(comp_recalls)}')\n",
    "print(f'Average comp F1: {np.mean(comp_f1s)}')\n",
    "print(f'Average FRI precision: {np.mean(fri_precs)}')\n",
    "print(f'Average FRI recall: {np.mean(fri_recalls)}')\n",
    "print(f'Average FRI F1: {np.mean(fri_f1s)}')\n",
    "print(f'Average FRII precision: {np.mean(frii_precs)}')\n",
    "print(f'Average FRII recall: {np.mean(frii_recalls)}')\n",
    "print(f'Average FRII F1: {np.mean(frii_f1s)}')\n",
    "print(f'Average Macro F1: {np.mean(macro_f1s)}')\n",
    "plot_cm(average_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heavier auxiliary weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 20\n",
    "total_loss = 0\n",
    "total_lbl_entropy = 0\n",
    "total_bent_entropy = 0\n",
    "total_fr_mse = 0\n",
    "total_cores_mse = 0\n",
    "total_size_mse = 0\n",
    "total_lbl_acc = 0\n",
    "total_bent_acc = 0\n",
    "total_fr_mae = 0\n",
    "total_cores_mae = 0\n",
    "total_size_mae = 0\n",
    "elapsed = 0\n",
    "\n",
    "# Execute a few runs of training the network\n",
    "for run in range(1,runs+1):\n",
    "    start = time.time()\n",
    "    tmp_loss, tmp_lbl_entropy, tmp_bent_entropy, tmp_fr_mse, tmp_cores_mse, tmp_size_mse, tmp_lbl_acc, tmp_bent_acc, tmp_fr_mae, tmp_cores_mae, tmp_size_mae = train_merged(X_train, y_train, y_aux_train,\n",
    "                                          X_val, y_val, y_aux_val, X_test, y_test, y_aux_test, 0.25, 0.75, run)\n",
    "    keras.backend.clear_session()\n",
    "    end = time.time()\n",
    "    elapsed += (end - start)\n",
    "    total_loss += tmp_loss\n",
    "    total_lbl_entropy += tmp_lbl_entropy\n",
    "    total_bent_entropy += tmp_bent_entropy\n",
    "    total_fr_mse += tmp_fr_mse\n",
    "    total_cores_mse += tmp_cores_mse\n",
    "    total_size_mse += tmp_size_mse\n",
    "    total_lbl_acc += tmp_lbl_acc\n",
    "    total_bent_acc += tmp_bent_acc\n",
    "    total_fr_mae += tmp_fr_mae\n",
    "    total_cores_mae += tmp_cores_mae\n",
    "    total_size_mae += tmp_size_mae\n",
    "avg_loss = total_loss/runs\n",
    "avg_lbl_entropy = total_lbl_entropy/runs\n",
    "avg_bent_entropy = total_bent_entropy/runs\n",
    "avg_fr_mse = total_fr_mse/runs\n",
    "avg_cores_mse = total_cores_mse/runs\n",
    "avg_size_mse = total_size_mse/runs\n",
    "avg_lbl_acc = total_lbl_acc/runs\n",
    "avg_bent_acc = total_bent_acc/runs\n",
    "avg_fr_mae = total_fr_mae/runs\n",
    "avg_cores_mae = total_cores_mae/runs\n",
    "avg_size_mae = total_size_mae/runs\n",
    "avg_elapsed = elapsed/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 20\n",
    "epochs_log = []\n",
    "for run in range(1,runs+1):\n",
    "    ea = EventAccumulator(f'../../lr_logs/merged_0.25_0.75_run{run}/train')\n",
    "    ea.Reload()\n",
    "    ct_loss, epochs_loss, loss = zip(*ea.Tensors('epoch_loss'))\n",
    "    epochs_loss = np.asarray(epochs_loss)\n",
    "    epochs_log.append(epochs_loss[-1] + 1)\n",
    "\n",
    "epochs_log = np.array(epochs_log)\n",
    "avg_epochs = np.mean(epochs_log)\n",
    "min_idx = np.argmin(epochs_log) + 1\n",
    "min_epochs = epochs_log[min_idx - 1]\n",
    "max_idx = np.argmax(epochs_log) + 1\n",
    "max_epochs = epochs_log[max_idx - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = (avg_fr_mse + avg_cores_mse + avg_size_mse)/3\n",
    "avg_mae = (avg_fr_mae + avg_cores_mae + avg_size_mae)/3\n",
    "tmp = np.array([avg_lbl_entropy, avg_bent_entropy, avg_fr_mse, avg_cores_mse, avg_size_mse, avg_lbl_acc, avg_bent_acc, avg_fr_mae, avg_cores_mae, avg_size_mae, avg_mse, avg_mae, avg_loss, avg_elapsed, avg_epochs, max_epochs, max_idx, min_epochs, min_idx])\n",
    "probe_dir('../../results/')\n",
    "np.save('../../results/merged_aux_results.npy', tmp)\n",
    "tmp = np.load('../../results/merged_aux_results.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average classification entropy: {tmp[0]}\")\n",
    "print(f\"Average bent entropy: {tmp[1]}\")\n",
    "print(f\"Average FR ratio MSE: {tmp[2]}\")\n",
    "print(f\"Average core count MSE: {tmp[3]}\")\n",
    "print(f\"Average core ratio MSE: {tmp[4]}\")\n",
    "print(f\"Average classification accuracy: {tmp[5]}\")\n",
    "print(f\"Average bent accuracy: {tmp[6]}\")\n",
    "print(f\"Average FR ratio MAE: {tmp[7]}\")\n",
    "print(f\"Average core count MAE: {tmp[8]}\")\n",
    "print(f\"Average core ratio MAE: {tmp[9]}\")\n",
    "print(f\"Average overall MSE: {tmp[10]}\")\n",
    "print(f\"Average overall MAE: {tmp[11]}\")\n",
    "print(f\"Average loss: {tmp[12]}\")\n",
    "print(f\"Average time taken: {tmp[13]}\")\n",
    "print(f\"Average epochs taken: {tmp[14]}\")\n",
    "print(f\"Maximum number of epochs taken was {tmp[15]} at run {tmp[16]}\")\n",
    "print(f\"Minimum number of epochs taken was {tmp[17]} at run {tmp[18]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bent_precs, bent_recalls, bent_f1s = [], [], []\n",
    "comp_precs, comp_recalls, comp_f1s = [], [], []\n",
    "fri_precs, fri_recalls, fri_f1s = [], [], []\n",
    "frii_precs, frii_recalls, frii_f1s = [], [], []\n",
    "macro_f1s = []\n",
    "average_cm = np.zeros((4, 4))\n",
    "runs = 20\n",
    "for run in range(1,runs+1):\n",
    "    best_model = keras.models.load_model(f\"../../models/merged_0.25_0.75_model{run}.h5\")\n",
    "    test_pred = extract_class_pred(best_model.predict(X_test))\n",
    "    new_test_true, new_test_pred = process_labels(y_test, test_pred)\n",
    "    cm = confusion_matrix(new_test_true, new_test_pred)\n",
    "    average_cm += cm\n",
    "    bent_prec, bent_recall = calc_precision_recall(cm, 0)\n",
    "    bent_precs.append(bent_prec)\n",
    "    bent_recalls.append(bent_recall)\n",
    "    bent_f1s.append(calc_f1(bent_prec, bent_recall))\n",
    "    comp_prec, comp_recall = calc_precision_recall(cm, 1)\n",
    "    comp_precs.append(comp_prec)\n",
    "    comp_recalls.append(comp_recall)\n",
    "    comp_f1s.append(calc_f1(comp_prec, comp_recall))\n",
    "    fri_prec, fri_recall = calc_precision_recall(cm, 2)\n",
    "    fri_precs.append(fri_prec)\n",
    "    fri_recalls.append(fri_recall)\n",
    "    fri_f1s.append(calc_f1(fri_prec, fri_recall))\n",
    "    frii_prec, frii_recall = calc_precision_recall(cm, 3)\n",
    "    frii_precs.append(frii_prec)\n",
    "    frii_recalls.append(frii_recall)\n",
    "    frii_f1s.append(calc_f1(frii_prec, frii_recall))\n",
    "    macro_f1s.append((bent_f1s[-1] + comp_f1s[-1] + fri_f1s[-1] + frii_f1s[-1])/4)\n",
    "\n",
    "average_cm = average_cm/runs\n",
    "print(f'Average bent precision: {np.mean(bent_precs)}')\n",
    "print(f'Average bent recall: {np.mean(bent_recalls)}')\n",
    "print(f'Average bent F1: {np.mean(bent_f1s)}')\n",
    "print(f'Average comp precision: {np.mean(comp_precs)}')\n",
    "print(f'Average comp recall: {np.mean(comp_recalls)}')\n",
    "print(f'Average comp F1: {np.mean(comp_f1s)}')\n",
    "print(f'Average FRI precision: {np.mean(fri_precs)}')\n",
    "print(f'Average FRI recall: {np.mean(fri_recalls)}')\n",
    "print(f'Average FRI F1: {np.mean(fri_f1s)}')\n",
    "print(f'Average FRII precision: {np.mean(frii_precs)}')\n",
    "print(f'Average FRII recall: {np.mean(frii_recalls)}')\n",
    "print(f'Average FRII F1: {np.mean(frii_f1s)}')\n",
    "print(f'Average Macro F1: {np.mean(macro_f1s)}')\n",
    "plot_cm(average_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heavier main weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heavier main head\n",
    "runs = 20\n",
    "total_loss = 0\n",
    "total_lbl_entropy = 0\n",
    "total_bent_entropy = 0\n",
    "total_fr_mse = 0\n",
    "total_cores_mse = 0\n",
    "total_size_mse = 0\n",
    "total_lbl_acc = 0\n",
    "total_bent_acc = 0\n",
    "total_fr_mae = 0\n",
    "total_cores_mae = 0\n",
    "total_size_mae = 0\n",
    "elapsed = 0\n",
    "\n",
    "# Execute a few runs of training the network\n",
    "for run in range(1,runs+1):\n",
    "    start = time.time()\n",
    "    tmp_loss, tmp_lbl_entropy, tmp_bent_entropy, tmp_fr_mse, tmp_cores_mse, tmp_size_mse, tmp_lbl_acc, tmp_bent_acc, tmp_fr_mae, tmp_cores_mae, tmp_size_mae = train_galaxy_merged(X_train, y_train, y_aux_train,\n",
    "                                          X_val, y_val, y_aux_val, X_test, y_test, y_aux_test, 0.75, 0.25, run)\n",
    "    keras.backend.clear_session()\n",
    "    end = time.time()\n",
    "    elapsed += (end - start)\n",
    "    total_loss += tmp_loss\n",
    "    total_lbl_entropy += tmp_lbl_entropy\n",
    "    total_bent_entropy += tmp_bent_entropy\n",
    "    total_fr_mse += tmp_fr_mse\n",
    "    total_cores_mse += tmp_cores_mse\n",
    "    total_size_mse += tmp_size_mse\n",
    "    total_lbl_acc += tmp_lbl_acc\n",
    "    total_bent_acc += tmp_bent_acc\n",
    "    total_fr_mae += tmp_fr_mae\n",
    "    total_cores_mae += tmp_cores_mae\n",
    "    total_size_mae += tmp_size_mae\n",
    "avg_loss = total_loss/runs\n",
    "avg_lbl_entropy = total_lbl_entropy/runs\n",
    "avg_bent_entropy = total_bent_entropy/runs\n",
    "avg_fr_mse = total_fr_mse/runs\n",
    "avg_cores_mse = total_cores_mse/runs\n",
    "avg_size_mse = total_size_mse/runs\n",
    "avg_lbl_acc = total_lbl_acc/runs\n",
    "avg_bent_acc = total_bent_acc/runs\n",
    "avg_fr_mae = total_fr_mae/runs\n",
    "avg_cores_mae = total_cores_mae/runs\n",
    "avg_size_mae = total_size_mae/runs\n",
    "avg_elapsed = elapsed/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 20\n",
    "epochs_log = []\n",
    "for run in range(1,runs+1):\n",
    "    ea = EventAccumulator(f'../../lr_logs/merged_0.75_0.25_run{run}/train')\n",
    "    ea.Reload()\n",
    "    ct_loss, epochs_loss, loss = zip(*ea.Tensors('epoch_loss'))\n",
    "    epochs_loss = np.asarray(epochs_loss)\n",
    "    epochs_log.append(epochs_loss[-1] + 1)\n",
    "\n",
    "epochs_log = np.array(epochs_log)\n",
    "avg_epochs = np.mean(epochs_log)\n",
    "min_idx = np.argmin(epochs_log) + 1\n",
    "min_epochs = epochs_log[min_idx - 1]\n",
    "max_idx = np.argmax(epochs_log) + 1\n",
    "max_epochs = epochs_log[max_idx - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mse = (avg_fr_mse + avg_cores_mse + avg_size_mse)/3\n",
    "avg_mae = (avg_fr_mae + avg_cores_mae + avg_size_mae)/3\n",
    "tmp = np.array([avg_lbl_entropy, avg_bent_entropy, avg_fr_mse, avg_cores_mse, avg_size_mse, avg_lbl_acc, avg_bent_acc, avg_fr_mae, avg_cores_mae, avg_size_mae, avg_mse, avg_mae, avg_loss, avg_elapsed, avg_epochs, max_epochs, max_idx, min_epochs, min_idx])\n",
    "probe_dir('../../results/')\n",
    "np.save('../../results/merged_main_results.npy', tmp)\n",
    "tmp = np.load('../../results/merged_main_results.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average classification entropy: {tmp[0]}\")\n",
    "print(f\"Average bent entropy: {tmp[1]}\")\n",
    "print(f\"Average FR ratio MSE: {tmp[2]}\")\n",
    "print(f\"Average core count MSE: {tmp[3]}\")\n",
    "print(f\"Average core ratio MSE: {tmp[4]}\")\n",
    "print(f\"Average classification accuracy: {tmp[5]}\")\n",
    "print(f\"Average bent accuracy: {tmp[6]}\")\n",
    "print(f\"Average FR ratio MAE: {tmp[7]}\")\n",
    "print(f\"Average core count MAE: {tmp[8]}\")\n",
    "print(f\"Average core ratio MAE: {tmp[9]}\")\n",
    "print(f\"Average overall MSE: {tmp[10]}\")\n",
    "print(f\"Average overall MAE: {tmp[11]}\")\n",
    "print(f\"Average loss: {tmp[12]}\")\n",
    "print(f\"Average time taken: {tmp[13]}\")\n",
    "print(f\"Average epochs taken: {tmp[14]}\")\n",
    "print(f\"Maximum number of epochs taken was {tmp[15]} at run {tmp[16]}\")\n",
    "print(f\"Minimum number of epochs taken was {tmp[17]} at run {tmp[18]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bent_precs, bent_recalls, bent_f1s = [], [], []\n",
    "comp_precs, comp_recalls, comp_f1s = [], [], []\n",
    "fri_precs, fri_recalls, fri_f1s = [], [], []\n",
    "frii_precs, frii_recalls, frii_f1s = [], [], []\n",
    "macro_f1s = []\n",
    "average_cm = np.zeros((4, 4))\n",
    "runs = 20\n",
    "for run in range(1,runs+1):\n",
    "    best_model = keras.models.load_model(f\"../../models/merged_0.75_0.25_model{run}.h5\")\n",
    "    test_pred = extract_class_pred(best_model.predict(X_test))\n",
    "    new_test_true, new_test_pred = process_labels(y_test, test_pred)\n",
    "    cm = confusion_matrix(new_test_true, new_test_pred)\n",
    "    average_cm += cm\n",
    "    bent_prec, bent_recall = calc_precision_recall(cm, 0)\n",
    "    bent_precs.append(bent_prec)\n",
    "    bent_recalls.append(bent_recall)\n",
    "    bent_f1s.append(calc_f1(bent_prec, bent_recall))\n",
    "    comp_prec, comp_recall = calc_precision_recall(cm, 1)\n",
    "    comp_precs.append(comp_prec)\n",
    "    comp_recalls.append(comp_recall)\n",
    "    comp_f1s.append(calc_f1(comp_prec, comp_recall))\n",
    "    fri_prec, fri_recall = calc_precision_recall(cm, 2)\n",
    "    fri_precs.append(fri_prec)\n",
    "    fri_recalls.append(fri_recall)\n",
    "    fri_f1s.append(calc_f1(fri_prec, fri_recall))\n",
    "    frii_prec, frii_recall = calc_precision_recall(cm, 3)\n",
    "    frii_precs.append(frii_prec)\n",
    "    frii_recalls.append(frii_recall)\n",
    "    frii_f1s.append(calc_f1(frii_prec, frii_recall))\n",
    "    macro_f1s.append((bent_f1s[-1] + comp_f1s[-1] + fri_f1s[-1] + frii_f1s[-1])/4)\n",
    "\n",
    "average_cm = average_cm/runs\n",
    "print(f'Average bent precision: {np.mean(bent_precs)}')\n",
    "print(f'Average bent recall: {np.mean(bent_recalls)}')\n",
    "print(f'Average bent F1: {np.mean(bent_f1s)}')\n",
    "print(f'Average comp precision: {np.mean(comp_precs)}')\n",
    "print(f'Average comp recall: {np.mean(comp_recalls)}')\n",
    "print(f'Average comp F1: {np.mean(comp_f1s)}')\n",
    "print(f'Average FRI precision: {np.mean(fri_precs)}')\n",
    "print(f'Average FRI recall: {np.mean(fri_recalls)}')\n",
    "print(f'Average FRI F1: {np.mean(fri_f1s)}')\n",
    "print(f'Average FRII precision: {np.mean(frii_precs)}')\n",
    "print(f'Average FRII recall: {np.mean(frii_recalls)}')\n",
    "print(f'Average FRII F1: {np.mean(frii_f1s)}')\n",
    "print(f'Average Macro F1: {np.mean(macro_f1s)}')\n",
    "plot_cm(average_cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
